import streamlit as st
import os
from dotenv import load_dotenv

# LangChain & Groq
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Embeddings & Vector DB
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# Load environment variables
load_dotenv()
os.environ["HF_TOKEN"] = os.getenv("HF_TOKEN")

# --- Hardcoded Groq API key ---
GROQ_API_KEY = "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"

# ---- Streamlit Layout ----
st.set_page_config(page_title="Groq PDF RAG (Nepali)", layout="wide")
st.title("ЁЯУД Groq PDF Q&A (рдиреЗрдкрд╛рд▓реА)")
st.write("рдкреВрд░реНрд╡-рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ PDF embeddings рдмрд╛рдЯ рдЙрддреНрддрд░ рджрд┐рдиреНрдЫред")

# ---- Load persisted vectorstore from repo ----
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
VECTORSTORE_DIR = "./chroma_db"  # ЁЯСИ This folder must be committed to repo

if os.path.exists(VECTORSTORE_DIR):
    vectorstore = Chroma(
        embedding_function=embeddings,
        persist_directory=VECTORSTORE_DIR
    )
    retriever = vectorstore.as_retriever()
    st.success("тЬЕ Vectorstore loaded successfully")
else:
    st.error(f"тЪая╕П Persisted vectorstore рдлреЛрд▓реНрдбрд░ рднреЗрдЯрд┐рдПрди: {VECTORSTORE_DIR}")
    retriever = None

# ---- User input & RAG ----
if retriever:
    if prompt := st.chat_input("рдкреВрд░реНрд╡-рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ PDF рдмрд╛рд░реЗ рдкреНрд░рд╢реНрди рд╕реЛрдзреНрдиреБрд╣реЛрд╕реН..."):
        st.chat_message("user").write(prompt)

        # Initialize Groq LLM
        llm = ChatGroq(api_key=GROQ_API_KEY, model="llama-3.3-70b-versatile")

        # System prompt enforcing Nepali-only responses
        system_prompt = (
            "рддрдкрд╛рдИрдБ рдПрдЙрдЯрд╛ рд╕рд╣рд╛рдпрдХ рд╕рд╣рд╛рдпрдХ рд╣реБрдиреБрд╣реБрдиреНрдЫред "
            "**рдХреБрдиреИ рдкрдирд┐ рд╣рд╛рд▓рддрдорд╛ рд╣рд┐рдиреНрджреА рд╡рд╛ рдЕрдиреНрдп рднрд╛рд╖рд╛ рдкреНрд░рдпреЛрдЧ рдирдЧрд░реНрдиреБрд╣реЛрд╕реНред** "
            "**рд╕рдзреИрдВ рдХреЗрд╡рд▓ рдиреЗрдкрд╛рд▓реАрдорд╛ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред** "
            "рджрд┐рдЗрдПрдХреЛ рдкреНрд░рд╕рдЩреНрдЧ (context) рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред "
            "рдпрджрд┐ рдкреНрд░рд╕рдЩреНрдЧ рдЦрд╛рд▓реА рдЫ рд╡рд╛ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЫреИрди рднрдиреЗ 'рдорд▓рд╛рдИ рдерд╛рд╣рд╛ рдЫреИрди' рднрдиреНрдиреБрд╣реЛрд╕реНред "
            "рдЬрд╡рд╛рдл рдЫреЛрдЯрдХрд░реАрдорд╛ рджрд┐рдиреБрд╣реЛрд╕реН (рдЕрдзрд┐рдХрддрдо рео рд╡рд╛рдХреНрдпрд╕рдореНрдо)ред\n\n{context}"
        )

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{input}"),
            ]
        )

        # Create RAG chain
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Prepend explicit Nepali instruction to user input
        user_input = "рдиреЗрдкрд╛рд▓реАрдорд╛ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реН: " + prompt

        # Run RAG
        rag_response = rag_chain.invoke({"input": user_input})
        answer = rag_response["answer"]

        # Show response in Nepali
        st.chat_message("assistant").write(answer)
