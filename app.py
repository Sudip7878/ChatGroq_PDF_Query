import streamlit as st
import os
from dotenv import load_dotenv

# LangChain & Groq
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Embeddings & Vector DB
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

# -----------------------------
# Load environment / Streamlit secrets
# -----------------------------
load_dotenv()

HF_TOKEN = os.getenv("HF_TOKEN") or st.secrets.get("hf", {}).get("token")
GROQ_API_KEY = os.getenv("GROQ_API_KEY") or st.secrets.get("groq", {}).get("api_key")

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="Groq PDF RAG (Nepali)", layout="wide")
st.title("ЁЯУД Groq PDF Q&A (рдиреЗрдкрд╛рд▓реА)")
st.write("рдкреВрд░реНрд╡-рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ PDF embeddings рдмрд╛рдЯ рдЙрддреНрддрд░ рджрд┐рдиреНрдЫред")

# -----------------------------
# Load persisted vectorstore
# -----------------------------
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"device": "cpu"}  # use "cuda" if GPU is available
)

VECTORSTORE_DIR = "./chroma_db"  # must be included in repo

if os.path.exists(VECTORSTORE_DIR):
    vectorstore = Chroma(
        embedding_function=embeddings,
        persist_directory=VECTORSTORE_DIR
    )
    retriever = vectorstore.as_retriever()
    st.success("тЬЕ Vectorstore loaded successfully")
else:
    st.error(f"тЪая╕П Persisted vectorstore рдлреЛрд▓реНрдбрд░ рднреЗрдЯрд┐рдПрди: {VECTORSTORE_DIR}")
    retriever = None

# -----------------------------
# RAG Q&A Loop
# -----------------------------
if retriever:
    if prompt := st.chat_input("рдкреВрд░реНрд╡-рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ PDF рдмрд╛рд░реЗ рдкреНрд░рд╢реНрди рд╕реЛрдзреНрдиреБрд╣реЛрд╕реН..."):
        st.chat_message("user").write(prompt)

        # Initialize Groq LLM
        llm = ChatGroq(api_key=GROQ_API_KEY, model="llama-3.3-70b-versatile")

        # System prompt enforcing Nepali-only answers
        system_prompt = (
            "рддрдкрд╛рдИрдБ рдПрдЙрдЯрд╛ рд╕рд╣рд╛рдпрдХ рд╕рд╣рд╛рдпрдХ рд╣реБрдиреБрд╣реБрдиреНрдЫред "
            "рд╕рдзреИрдВ рдХреЗрд╡рд▓ рдиреЗрдкрд╛рд▓реА рднрд╛рд╖рд╛рдорд╛ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред "
            "рдХреБрдиреИ рдкрдирд┐ рд╣рд╛рд▓рддрдорд╛ рд╣рд┐рдиреНрджреА рд╡рд╛ рдЕрдиреНрдп рднрд╛рд╖рд╛ рдкреНрд░рдпреЛрдЧ рдирдЧрд░реНрдиреБрд╣реЛрд╕реНред "
            "рджрд┐рдЗрдПрдХреЛ рдкреНрд░рд╕рдЩреНрдЧ (context) рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред "
            "рдпрджрд┐ рдкреНрд░рд╕рдЩреНрдЧ рдЦрд╛рд▓реА рдЫ рд╡рд╛ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЫреИрди рднрдиреЗ 'рдорд▓рд╛рдИ рдерд╛рд╣рд╛ рдЫреИрди' рднрдиреНрдиреБрд╣реЛрд╕реНред "
            "рдЬрд╡рд╛рдл рдЫреЛрдЯрдХрд░реАрдорд╛ рджрд┐рдиреБрд╣реЛрд╕реН (рдЕрдзрд┐рдХрддрдо рео рд╡рд╛рдХреНрдпрд╕рдореНрдо)ред\n\n{context}"
        )

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{input}"),
            ]
        )

        # Build chain
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Prepend explicit Nepali instruction
        user_input = "рдиреЗрдкрд╛рд▓реАрдорд╛ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реН: " + prompt

        # Run RAG
        rag_response = rag_chain.invoke({"input": user_input})
        answer = rag_response["answer"]

        # Show answer
        st.chat_message("assistant").write(answer)
