import streamlit as st
import os

# LangChain & Groq
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Embeddings & Vector DB
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from huggingface_hub import login

# --- Load secrets from Streamlit Cloud ---
HF_TOKEN = st.secrets["hf"]["token"]
GROQ_API_KEY = st.secrets["groq"]["api_key"]

# Login to Hugging Face Hub
login(HF_TOKEN)

# ---- Streamlit Layout ----
st.set_page_config(page_title="Groq PDF RAG (Nepali)", layout="wide")
st.title("ЁЯУД Groq PDF Q&A (рдиреЗрдкрд╛рд▓реА)")
st.write("рдкреВрд░реНрд╡-рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ PDF embeddings рдмрд╛рдЯ рдЙрддреНрддрд░ рджрд┐рдиреНрдЫред")

# ---- Load persisted vectorstore from repo ----
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    model_kwargs={"use_auth_token": HF_TOKEN},
)

VECTORSTORE_DIR = "./chroma_db"  # ЁЯСИ must be in repo

if os.path.exists(VECTORSTORE_DIR):
    vectorstore = Chroma(
        embedding_function=embeddings,
        persist_directory=VECTORSTORE_DIR
    )
    retriever = vectorstore.as_retriever()
    st.success("тЬЕ Vectorstore loaded successfully")
else:
    st.error(f"тЪая╕П Persisted vectorstore рдлреЛрд▓реНрдбрд░ рднреЗрдЯрд┐рдПрди: {VECTORSTORE_DIR}")
    retriever = None

# ---- User input & RAG ----
if retriever:
    if prompt := st.chat_input("рдкреВрд░реНрд╡-рд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ PDF рдмрд╛рд░реЗ рдкреНрд░рд╢реНрди рд╕реЛрдзреНрдиреБрд╣реЛрд╕реН..."):
        st.chat_message("user").write(prompt)

        # Initialize Groq LLM
        llm = ChatGroq(api_key=GROQ_API_KEY, model="llama-3.3-70b-versatile")

        # System prompt enforcing Nepali-only responses
        system_prompt = (
            "рддрдкрд╛рдИрдБ рдПрдЙрдЯрд╛ рд╕рд╣рд╛рдпрдХ рд╕рд╣рд╛рдпрдХ рд╣реБрдиреБрд╣реБрдиреНрдЫред "
            "**рдХреБрдиреИ рдкрдирд┐ рд╣рд╛рд▓рддрдорд╛ рд╣рд┐рдиреНрджреА рд╡рд╛ рдЕрдиреНрдп рднрд╛рд╖рд╛ рдкреНрд░рдпреЛрдЧ рдирдЧрд░реНрдиреБрд╣реЛрд╕реНред** "
            "**рд╕рдзреИрдВ рдХреЗрд╡рд▓ рдиреЗрдкрд╛рд▓реАрдорд╛ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред** "
            "рджрд┐рдЗрдПрдХреЛ рдкреНрд░рд╕рдЩреНрдЧ (context) рдкреНрд░рдпреЛрдЧ рдЧрд░реЗрд░ рдорд╛рддреНрд░ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реНред "
            "рдпрджрд┐ рдкреНрд░рд╕рдЩреНрдЧ рдЦрд╛рд▓реА рдЫ рд╡рд╛ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЫреИрди рднрдиреЗ 'рдорд▓рд╛рдИ рдерд╛рд╣рд╛ рдЫреИрди' рднрдиреНрдиреБрд╣реЛрд╕реНред "
            "рдЬрд╡рд╛рдл рдЫреЛрдЯрдХрд░реАрдорд╛ рджрд┐рдиреБрд╣реЛрд╕реН (рдЕрдзрд┐рдХрддрдо рео рд╡рд╛рдХреНрдпрд╕рдореНрдо)ред\n\n{context}"
        )

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", system_prompt),
                ("human", "{input}"),
            ]
        )

        # Create RAG chain
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)

        # Force user input to Nepali
        user_input = "рдиреЗрдкрд╛рд▓реАрдорд╛ рдЬрд╡рд╛рдл рджрд┐рдиреБрд╣реЛрд╕реН: " + prompt

        # Run RAG
        rag_response = rag_chain.invoke({"input": user_input})
        answer = rag_response["answer"]

        # Show response
        st.chat_message("assistant").write(answer)
